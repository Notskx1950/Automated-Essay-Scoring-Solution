{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8141507,"sourceType":"datasetVersion","datasetId":4813598},{"sourceId":8247140,"sourceType":"datasetVersion","datasetId":4892997},{"sourceId":8388154,"sourceType":"datasetVersion","datasetId":4989058},{"sourceId":180985740,"sourceType":"kernelVersion"},{"sourceId":184867535,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nimport polars as pl\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport spacy\nimport string\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, HashingVectorizer\nfrom sklearn.metrics import roc_curve, cohen_kappa_score, accuracy_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n\nimport lightgbm as lgb\nfrom lightgbm import log_evaluation, early_stopping\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm.auto import tqdm,trange","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-27T09:03:08.373292Z","iopub.execute_input":"2024-06-27T09:03:08.373596Z","iopub.status.idle":"2024-06-27T09:03:19.194635Z","shell.execute_reply.started":"2024-06-27T09:03:08.373551Z","shell.execute_reply":"2024-06-27T09:03:19.193829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [(pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")),]\nPATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n\ntrain = pl.read_csv(\"/kaggle/input/aes2-train-data-with-prompt-and-is-kaggle-only/train.csv\")\ntrain = train.with_columns(columns)\n\ntest = pl.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\").with_columns(columns)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:03:19.196118Z","iopub.execute_input":"2024-06-27T09:03:19.196649Z","iopub.status.idle":"2024-06-27T09:03:19.804341Z","shell.execute_reply.started":"2024-06-27T09:03:19.196623Z","shell.execute_reply":"2024-06-27T09:03:19.803385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/english-word-hx/words.txt', 'r') as file:\n    english_vocab = set(word.strip().lower() for word in file)\ndef fast_count_spelling_errors(text):\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    spelling_errors = sum(1 for token in tokens if token not in english_vocab)\n    return spelling_errors","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:03:19.809065Z","iopub.execute_input":"2024-06-27T09:03:19.809388Z","iopub.status.idle":"2024-06-27T09:03:20.055386Z","shell.execute_reply.started":"2024-06-27T09:03:19.809362Z","shell.execute_reply":"2024-06-27T09:03:20.054428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cList = {\n  \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",  \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\n  \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n  \"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n  \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n  \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n  \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\n  \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n  \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",  \"you've\": \"you have\"\n   }\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:03:20.056892Z","iopub.execute_input":"2024-06-27T09:03:20.057203Z","iopub.status.idle":"2024-06-27T09:03:20.096362Z","shell.execute_reply.started":"2024-06-27T09:03:20.057177Z","shell.execute_reply":"2024-06-27T09:03:20.095293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeHTML(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_punctuation(text):\n    # string.punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\ndef dataPreprocessing(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove html\n    text = expandContractions(text)\n    text = removeHTML(text)\n    # Convert escaped single quotes to regular single quotes\n    text = text.replace(\"\\'\", \"'\")\n    # Remove strings starting with @\n    text = re.sub(\"@\\w+\", '', text)\n    # Remove URL\n    text = re.sub(\"http\\w+\", '', text)\n    # Replace consecutive empty spaces with a single space character\n    text = re.sub(r\"\\s+\", \" \", text)\n    # Replace consecutive commas and periods with one comma and period character\n    text = re.sub(r\"\\.+\", \".\", text)\n    text = re.sub(r\"\\,+\", \",\", text)\n    # Remove empty characters at the beginning and end\n    text = text.strip()\n    return text\n\ndef dataPreprocessingWithoutPunc(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove html\n    text = expandContractions(text)\n    text = removeHTML(text)\n    # Remove non-alphanumeric characters\n    #text = re.sub(r'\\W', ' ', text)\n    # Remove strings starting with @\n    text = re.sub(\"@\\w+\", '', text)\n    # Remove URL\n    text = re.sub(\"http\\w+\", '', text)\n    # Replace consecutive empty spaces with a single space character\n    text = re.sub(r\"\\s+\", \" \", text)\n    # Replace consecutive commas and periods with one comma and period character\n    text = remove_punctuation(text)\n    # Remove empty characters at the beginning and end\n    text = text.strip()\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:03:20.097634Z","iopub.execute_input":"2024-06-27T09:03:20.097919Z","iopub.status.idle":"2024-06-27T09:03:20.109791Z","shell.execute_reply.started":"2024-06-27T09:03:20.097876Z","shell.execute_reply":"2024-06-27T09:03:20.108944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_specific_punctuation_patterns(text):\n    periods_not_followed_by_space_capital = len(re.findall(r'\\.(?!\\s[A-Z])', text))\n    commas_not_followed_by_space_letter = len(re.findall(r',(?!\\s[a-zA-Z])', text))\n    commas_preceded_by_space_or_not_beside_char = len(re.findall(r'(\\s,)|(^,)|(,$)', text))\n    return periods_not_followed_by_space_capital, commas_not_followed_by_space_letter, commas_preceded_by_space_or_not_beside_char\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:03:20.110924Z","iopub.execute_input":"2024-06-27T09:03:20.114134Z","iopub.status.idle":"2024-06-27T09:03:20.123035Z","shell.execute_reply.started":"2024-06-27T09:03:20.114100Z","shell.execute_reply":"2024-06-27T09:03:20.122185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# paragraph features\ndef Paragraph_Preprocess(tmp):\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    \n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(remove_punctuation).alias('paragraph_no_punctuation'))\n    tmp = tmp.with_columns(pl.col('paragraph_no_punctuation').map_elements(fast_count_spelling_errors).alias(\"paragraph_error_num\"))\n        \n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x), return_dtype=pl.Int64).alias(\"paragraph_len\"))\n    \n    # Calculate the number of sentences and words in each paragraph\n    tmp = tmp.with_columns(\n        pl.col('paragraph').map_elements(lambda x: len(x.split('.')), return_dtype=pl.Int64).alias(\"paragraph_sentence_cnt\"),\n        pl.col('paragraph').map_elements(lambda x: len(x.split(' ')), return_dtype=pl.Int64).alias(\"paragraph_word_cnt\"),\n    )\n    \n    tmp = tmp.with_columns(\n        pl.col('paragraph').map_elements(lambda x: count_specific_punctuation_patterns(x)[0], return_dtype=pl.Int64).alias(\"periods_not_followed_by_space_capital\"),\n        pl.col('paragraph').map_elements(lambda x: count_specific_punctuation_patterns(x)[1], return_dtype=pl.Int64).alias(\"commas_not_followed_by_space_letter\"),\n        pl.col('paragraph').map_elements(lambda x: count_specific_punctuation_patterns(x)[2], return_dtype=pl.Int64).alias(\"commas_preceded_by_space_or_not_beside_char\")\n    )\n    return tmp\n\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt','paragraph_error_num']\ndef Paragraph_Eng(train_tmp):\n    num_list = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600]\n    num_list2 = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700]\n    aggs = [\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in num_list ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea],  \n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea],  \n        pl.col('commas_not_followed_by_space_letter').sum().alias('commas_not_followed_by_space_letter'),\n        pl.col('periods_not_followed_by_space_capital').sum().alias('periods_not_followed_by_space_capital'),\n        pl.col('commas_preceded_by_space_or_not_beside_char').sum().alias('commas_preceded_by_space_or_not_beside_char'),\n        ]\n    \n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\n# Assuming `train` is your DataFrame\ntmp = Paragraph_Preprocess(train)\ntrain_feats = Paragraph_Eng(tmp)\n#train_feats['score'] = train['score']\n\n# Obtain feature names\nfeature_names = list(filter(lambda x: x not in ['essay_id', 'score'], train_feats.columns))\nprint('Features Number: ', len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:03:20.124265Z","iopub.execute_input":"2024-06-27T09:03:20.125179Z","iopub.status.idle":"2024-06-27T09:04:00.480992Z","shell.execute_reply.started":"2024-06-27T09:03:20.125149Z","shell.execute_reply":"2024-06-27T09:04:00.480126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sentence feature\ndef Sentence_Preprocess(tmp):\n    # Preprocess full_text and use periods to segment sentences in the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    # Calculate the length of a sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    # Filter out the portion of data with a sentence length greater than 15\n    tmp = tmp.filter(pl.col('sentence_len')>=15)\n    # Count the number of words in each sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n    return tmp\n\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt']\ndef Sentence_Eng(train_tmp):\n    aggs = [\n        # Count the number of sentences with a length greater than i\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [0,15,50,100,150,200,250,300] ], \n        *[pl.col('sentence').filter(pl.col('sentence_len') <= i).count().alias(f\"sentence_<{i}_cnt\") for i in [15,50] ], \n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in sentence_fea],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in sentence_fea], \n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in sentence_fea], \n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Sentence_Preprocess(train)\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:04:00.482239Z","iopub.execute_input":"2024-06-27T09:04:00.482512Z","iopub.status.idle":"2024-06-27T09:04:14.410777Z","shell.execute_reply.started":"2024-06-27T09:04:00.482490Z","shell.execute_reply":"2024-06-27T09:04:14.409794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word feature\ndef Word_Preprocess(tmp):\n    # Preprocess full_text and use spaces to separate words from the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessingWithoutPunc).str.split(\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    # Calculate the length of each word\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    # Delete data with a word length of 0\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    return tmp\n\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n        # other\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        pl.col('word').n_unique().alias('unique_word_count'),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    \n    # Calculate spelling errors for unique words\n    unique_words = train_tmp.select(['essay_id', 'word']).unique()\n    unique_words = unique_words.groupby('essay_id').agg([\n        pl.col('word').apply(lambda words: fast_count_spelling_errors(\" \".join(words)), return_dtype=pl.Int32).alias('spelling_error_count')\n    ])\n    unique_words = unique_words.to_pandas()\n    \n    # Merge spelling error counts into main DataFrame\n    df = df.merge(unique_words, on='essay_id', how='left')\n    \n    # Calculate ratio of spelling errors to unique word count\n    #df['spelling_error_ratio'] = df['spelling_error_count'] / df['unique_word_count']\n    return df\n\ntmp = Word_Preprocess(train)\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\n#tmpc = Word_Preprocess(competition)\n#competition_feats = competition_feats.merge(Word_Eng(tmpc), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:04:14.413877Z","iopub.execute_input":"2024-06-27T09:04:14.414205Z","iopub.status.idle":"2024-06-27T09:04:36.149305Z","shell.execute_reply.started":"2024-06-27T09:04:14.414180Z","shell.execute_reply":"2024-06-27T09:04:36.148430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feats.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:04:36.150437Z","iopub.execute_input":"2024-06-27T09:04:36.150745Z","iopub.status.idle":"2024-06-27T09:04:36.156593Z","shell.execute_reply.started":"2024-06-27T09:04:36.150719Z","shell.execute_reply":"2024-06-27T09:04:36.155524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_cnt = CountVectorizer(\n        tokenizer=lambda x: x,\n        preprocessor=lambda x: x,\n        token_pattern=None,\n        strip_accents='unicode',\n        analyzer = 'word',\n        ngram_range=(2,3),\n        min_df=0.10,\n        max_df=0.85,\n)\ntrain_tfid = vectorizer_cnt.fit_transform([i for i in train['full_text']])\ndense_matrix = train_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [f'tfid_char_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:04:36.157738Z","iopub.execute_input":"2024-06-27T09:04:36.158016Z","iopub.status.idle":"2024-06-27T09:05:48.169005Z","shell.execute_reply.started":"2024-06-27T09:04:36.157993Z","shell.execute_reply":"2024-06-27T09:05:48.168033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feats = train_feats.merge(train.to_pandas(), on='essay_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:05:48.170541Z","iopub.execute_input":"2024-06-27T09:05:48.171258Z","iopub.status.idle":"2024-06-27T09:05:48.710968Z","shell.execute_reply.started":"2024-06-27T09:05:48.171218Z","shell.execute_reply":"2024-06-27T09:05:48.710141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = list(filter(lambda x: x not in ['essay_id','score','paragraph','full_text','prompt_name','group','is_kaggle_only'], train_feats.columns))","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:05:48.712071Z","iopub.execute_input":"2024-06-27T09:05:48.712386Z","iopub.status.idle":"2024-06-27T09:05:48.718195Z","shell.execute_reply.started":"2024-06-27T09:05:48.712361Z","shell.execute_reply":"2024-06-27T09:05:48.717221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(feature_names)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:05:48.719261Z","iopub.execute_input":"2024-06-27T09:05:48.719599Z","iopub.status.idle":"2024-06-27T09:05:48.730433Z","shell.execute_reply.started":"2024-06-27T09:05:48.719573Z","shell.execute_reply":"2024-06-27T09:05:48.729488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    #y_true = y_true + a\n    y_true = (y_true + a).round()\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\n\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\n\ndef qwk_param_calc(y):\n    a = y.mean()\n    b = (y ** 2).mean() - a**2\n    return np.round(a, 4), np.round(b, 4)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:05:48.731429Z","iopub.execute_input":"2024-06-27T09:05:48.731669Z","iopub.status.idle":"2024-06-27T09:05:48.739935Z","shell.execute_reply.started":"2024-06-27T09:05:48.731648Z","shell.execute_reply":"2024-06-27T09:05:48.739109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_select_wrapper():\n    \"\"\"\n    lgm\n    :param train\n    :param test\n    :return\n    \"\"\"\n    # Part 1.\n    print('feature_select_wrapper...')\n    features = feature_names\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n    folds = skf.split(train_feats, train_feats['score'].values)\n    fse = pd.Series(0, index=features)\n    models = []\n    for fold_id, (trn_idx, val_idx) in tqdm(enumerate(folds)):\n\n        train_data = train_feats.iloc[trn_idx]\n        val_data = train_feats.iloc[val_idx]\n\n        model = lgb.LGBMRegressor(\n                objective=qwk_obj,\n                metric='None',\n                learning_rate=0.08,\n                max_depth=9,\n                num_leaves=12,\n                colsample_bytree=0.19485571273463959,\n                reg_alpha=0.21347141500003425,\n                reg_lambda=0.7870554384846639,\n                n_estimators=600,\n                n_jobs=-1,\n                random_state=42,\n                verbosity=-1,\n                min_gain_to_split=0.01,\n                #class_weight='balanced',\n                extra_trees=True,\n        )\n        #a, b = qwk_param_calc(train_data[\"score\"])\n        \n        X_train = train_data[feature_names]\n        Y_train = train_data['score'] - a\n\n        X_val = val_data[feature_names]\n        Y_val = val_data['score'] - a\n        print('\\nFold_{} Training ================================\\n'.format(fold_id+1))\n        \n        lgb_model = model.fit(X_train, Y_train,\n                              eval_names=['train', 'valid'],\n                              eval_set=[(X_train, Y_train), (X_val, Y_val)],\n                              eval_metric=quadratic_weighted_kappa,\n                              callbacks=callbacks,)\n        \n        models.append(lgb_model)\n        pred_val = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration_)\n        pred_val = pred_val + a\n        pred_val = pred_val.clip(1, 6).round()\n        #predictions.append(pred_val)\n\n        fse += pd.Series(lgb_model.feature_importances_, features)  \n        break\n    # Part 4.\n    feature_select = fse.sort_values(ascending=False).index.tolist()[:575]\n    print('done')\n    return feature_select","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:05:48.741173Z","iopub.execute_input":"2024-06-27T09:05:48.741414Z","iopub.status.idle":"2024-06-27T09:05:48.754141Z","shell.execute_reply.started":"2024-06-27T09:05:48.741393Z","shell.execute_reply":"2024-06-27T09:05:48.753285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a, b = qwk_param_calc(train_feats['score'])\nnew_feature_names = feature_select_wrapper()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:05:48.755155Z","iopub.execute_input":"2024-06-27T09:05:48.755425Z","iopub.status.idle":"2024-06-27T09:06:25.531667Z","shell.execute_reply.started":"2024-06-27T09:05:48.755403Z","shell.execute_reply":"2024-06-27T09:06:25.530901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = new_feature_names","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:06:25.532810Z","iopub.execute_input":"2024-06-27T09:06:25.533264Z","iopub.status.idle":"2024-06-27T09:06:25.536763Z","shell.execute_reply.started":"2024-06-27T09:06:25.533220Z","shell.execute_reply":"2024-06-27T09:06:25.535768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\n\ndef objective(trial):\n    # Suggest hyperparameters\n    learning_rate = trial.suggest_float(\"learning_rate\", 0.07, 0.1, log=True)\n    #num_leaves = trial.suggest_int(\"num_leaves\", 3, 16, log=True)\n    #max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n    #colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    #bagging_fraction=trial.suggest_float(\"bagging_fraction\", 0.1, 1.0),\n    #reg_alpha = trial.suggest_float(\"reg_alpha\", 0.0, 1.0)\n    #reg_lambda = trial.suggest_float(\"reg_lambda\", 0.0, 1.0)\n    #n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n    #min_gain_to_split = trial.suggest_float(\"min_gain_to_split\",0,20)\n    \n    oof = []\n    skf = StratifiedKFold(n_splits=7, random_state=42, shuffle=True)\n    callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=200,first_metric_only=True)]\n    folds = skf.split(train_feats, train_feats['score'].values)\n    \n    for fold_id, (trn_idx, val_idx) in tqdm(enumerate(folds)):\n        train_data = train_feats.iloc[trn_idx]\n        val_data = train_feats.iloc[val_idx]\n        # Create the model\n        model = lgb.LGBMRegressor(\n            objective=qwk_obj,\n            metric='None',\n            learning_rate=learning_rate,\n            max_depth=9,\n            num_leaves=12,\n            colsample_bytree=0.19485571273463959,\n            reg_alpha=0.21347141500003425,\n            reg_lambda=0.7870554384846639,\n            n_estimators=1200,\n            n_jobs=-1,\n            random_state=42,\n            verbosity=-1,\n            min_gain_to_split=0.01,\n            #class_weight='balanced',\n            extra_trees=True,\n        )\n        a, b = qwk_param_calc(train_data[\"score\"])\n        # Prepare training and validation sets\n        X_train = train_data[feature_names]\n        Y_train = train_data['score'] - a\n        \n        X_val = val_data[feature_names]\n        Y_val = val_data['score'] - a\n        print('\\nFold_{} Training ================================\\n'.format(fold_id+1))\n        # Train the model\n        lgb_model = model.fit(\n            X_train, Y_train,\n            eval_names=['train', 'valid'],\n            eval_set=[(X_train, Y_train), (X_val, Y_val)],\n            eval_metric=quadratic_weighted_kappa,\n            callbacks=callbacks,\n        )\n\n        # Predict on the validation set\n        pred_val = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration_)\n\n        # Store the predictions\n        df_tmp = val_data[['essay_id', 'score']].copy()\n        df_tmp['pred'] = pred_val + a\n        oof.append(df_tmp)\n    \n    df_oof = pd.concat(oof)\n    \n    # Calculate the evaluation metric\n    kappa = cohen_kappa_score(df_oof['score'], df_oof['pred'].clip(1, 6).round(), weights=\"quadratic\")\n    return kappa","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:06:25.538240Z","iopub.execute_input":"2024-06-27T09:06:25.538598Z","iopub.status.idle":"2024-06-27T09:06:25.552619Z","shell.execute_reply.started":"2024-06-27T09:06:25.538573Z","shell.execute_reply":"2024-06-27T09:06:25.551655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create and optimize the Optuna study\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Best hyperparameters: \", study.best_params)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:06:25.553808Z","iopub.execute_input":"2024-06-27T09:06:25.554271Z","iopub.status.idle":"2024-06-27T09:07:52.741714Z","shell.execute_reply.started":"2024-06-27T09:06:25.554239Z","shell.execute_reply":"2024-06-27T09:07:52.740054Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
